{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xinhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import re\n",
    "import string\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "global_seed = 123\n",
    "np.random.seed(global_seed)\n",
    "tf.random.set_seed(global_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2479</th>\n",
       "      <td>RANK: SGT/E-5 NON- COMMISSIONED OFFIC...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2480</th>\n",
       "      <td>GOVERNMENT RELATIONS, COMMUNICATIONS ...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>GEEK SQUAD AGENT         Professional...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>PROGRAM DIRECTOR / OFFICE MANAGER    ...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>STOREKEEPER II       Professional Sum...</td>\n",
       "      <td>AVIATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Resume_str  Category\n",
       "2479           RANK: SGT/E-5 NON- COMMISSIONED OFFIC...  AVIATION\n",
       "2480           GOVERNMENT RELATIONS, COMMUNICATIONS ...  AVIATION\n",
       "2481           GEEK SQUAD AGENT         Professional...  AVIATION\n",
       "2482           PROGRAM DIRECTOR / OFFICE MANAGER    ...  AVIATION\n",
       "2483           STOREKEEPER II       Professional Sum...  AVIATION"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"C:/Clubs and Society/DevHack/DevHack/dataset/Resume/Resume.csv\")\n",
    "df2 = df2[['Resume_str', 'Category']]\n",
    "df2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_paragraph(paragraph):\n",
    "    # Remove newlines\n",
    "    paragraph = paragraph.replace('\\n', ' ')\n",
    "\n",
    "    # Remove connectors (modify this list as needed)\n",
    "    connectors = ['and', 'but', 'or']\n",
    "    for connector in connectors:\n",
    "        paragraph = re.sub(r'\\b' + re.escape(connector) + r'\\b', '', paragraph, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove punctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    paragraph = paragraph.translate(translator)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resume</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HR ADMINISTRATORMARKETING ASSOCIATE HR ADMINIS...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HR SPECIALIST US HR OPERATIONS Summary Versati...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HR DIRECTOR Summary Over 20 years experience i...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HR SPECIALIST Summary Dedicated Driven Dynamic...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HR MANAGER Skill Highlights HR SKILLS HR Depar...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              resume Category\n",
       "0  HR ADMINISTRATORMARKETING ASSOCIATE HR ADMINIS...       HR\n",
       "1  HR SPECIALIST US HR OPERATIONS Summary Versati...       HR\n",
       "2  HR DIRECTOR Summary Over 20 years experience i...       HR\n",
       "3  HR SPECIALIST Summary Dedicated Driven Dynamic...       HR\n",
       "4  HR MANAGER Skill Highlights HR SKILLS HR Depar...       HR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['resume'] = df2['Resume_str'].apply(preprocess_paragraph)\n",
    "df1 = df2[['resume', 'Category']]\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train, Test and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1738,)\n",
      "Y_train: (1738,)\n",
      "X_test: (373,)\n",
      "Y_test: (373,)\n",
      "X_valid: (373,)\n",
      "Y_valid: (373,)\n"
     ]
    }
   ],
   "source": [
    "df = df1 \n",
    "X_train, x_test, y_train, y_test = train_test_split(df['resume'], df['Category'], test_size = 0.3, random_state = 42)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(x_test, y_test, test_size = 0.5, random_state = 42)\n",
    "\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('Y_train: ' + str(y_train.shape))\n",
    "print('X_test: ' + str(X_test.shape))\n",
    "print('Y_test: ' + str(y_test.shape))\n",
    "print('X_valid: ' + str(X_valid.shape))\n",
    "print('Y_valid: ' + str(y_valid.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train = label_encoder.fit_transform(y_train)\n",
    "Y_test = label_encoder.fit_transform(y_test)\n",
    "Y_valid = label_encoder.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading models from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \n",
    "                                                              #\"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \n",
    "                                                              #\"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \n",
    "                                                              #\"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_mask', 'input_word_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1\n",
      "Pooled Outputs Shape:(1, 128)\n",
      "Pooled Outputs Values:[-0.9999946   0.14341967 -0.998978    0.9951447  -0.99974567  0.9131291\n",
      " -0.996158   -0.970975    0.09740017  0.01907059 -0.8488176  -0.09385555]\n",
      "Sequence Outputs Shape:(1, 128, 128)\n",
      "Sequence Outputs Values:[[-0.43723688 -1.0138297  -2.374474   ... -0.75897044 -2.0910316\n",
      "  -0.25809968]\n",
      " [-1.3369719  -0.3688029   0.57868755 ... -2.1810174  -1.753092\n",
      "  -0.09555102]\n",
      " [-1.0607055  -0.30371076  0.34461164 ... -1.2898594  -1.9519897\n",
      "  -0.1223038 ]\n",
      " ...\n",
      " [-0.8602275  -0.54406166  0.6583735  ... -1.4765682  -1.7518609\n",
      "   1.1197377 ]\n",
      " [-0.46869135 -0.5715234   0.67256564 ... -1.7302349  -1.9740777\n",
      "   0.97479314]\n",
      " [ 0.02036636 -0.75511944  0.5948237  ... -1.9656786  -1.9051423\n",
      "   0.48970205]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "\n",
    "    # Additional dense layers with batch normalization\n",
    "    #net = tf.keras.layers.Dense(512, activation='tanh')(net)\n",
    "    #net = tf.keras.layers.BatchNormalization()(net)\n",
    "    #net = tf.keras.layers.Dropout(0.3)(net)\n",
    "\n",
    "    net = tf.keras.layers.Dense(256, activation='tanh')(net)\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "    \n",
    "    net = tf.keras.layers.Dense(128, activation='tanh')(net)\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Dropout(0.3)(net)\n",
    "\n",
    "    #net = tf.keras.layers.Dense(64, activation='tanh')(net)\n",
    "    #net = tf.keras.layers.BatchNormalization()(net)\n",
    "    #net = tf.keras.layers.Dropout(0.3)(net)\n",
    "    \n",
    "    # Softmax for multi-class classification\n",
    "    net = tf.keras.layers.Dense(24, activation='softmax', name='classifier')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.51074636 0.51410097 0.50611466 0.5107721  0.5113443  0.5105666\n",
      "  0.51517767 0.50286186 0.5119426  0.5065388  0.5069107  0.5045378\n",
      "  0.50368005 0.51623136 0.5073685  0.5106455  0.50361    0.50352424\n",
      "  0.5113672  0.5346103  0.512977   0.50880814 0.5118972  0.51357234]], shape=(1, 24), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "classifier_model.compile(optimizer = opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas Series to numpy arrays\n",
    "X_train_np = np.array(X_train)\n",
    "Y_train_np = np.array(Y_train)\n",
    "\n",
    "# Convert validation data if needed\n",
    "X_valid_np = np.array(X_valid)\n",
    "Y_valid_np = np.array(Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 28s 415ms/step - loss: 3.2432 - accuracy: 0.1893 - val_loss: 1.6155 - val_accuracy: 0.6032\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 25s 448ms/step - loss: 1.7441 - accuracy: 0.5472 - val_loss: 1.1833 - val_accuracy: 0.7212\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 27s 501ms/step - loss: 1.3229 - accuracy: 0.6640 - val_loss: 1.2934 - val_accuracy: 0.6917\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 28s 518ms/step - loss: 1.1213 - accuracy: 0.7227 - val_loss: 0.9880 - val_accuracy: 0.7587\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 27s 493ms/step - loss: 1.0033 - accuracy: 0.7560 - val_loss: 1.2439 - val_accuracy: 0.7239\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 27s 486ms/step - loss: 0.8507 - accuracy: 0.7871 - val_loss: 1.3449 - val_accuracy: 0.6971\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 26s 474ms/step - loss: 0.7236 - accuracy: 0.8147 - val_loss: 1.2085 - val_accuracy: 0.7480\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 26s 478ms/step - loss: 0.5832 - accuracy: 0.8504 - val_loss: 1.3773 - val_accuracy: 0.7534\n",
      "Epoch 9/20\n",
      "55/55 [==============================] - 26s 478ms/step - loss: 0.4772 - accuracy: 0.8728 - val_loss: 1.3180 - val_accuracy: 0.7453\n",
      "Epoch 10/20\n",
      "55/55 [==============================] - 27s 497ms/step - loss: 0.3473 - accuracy: 0.9183 - val_loss: 1.7662 - val_accuracy: 0.6729\n",
      "Epoch 11/20\n",
      "55/55 [==============================] - 26s 476ms/step - loss: 0.2808 - accuracy: 0.9275 - val_loss: 1.3476 - val_accuracy: 0.7828\n",
      "Epoch 12/20\n",
      "55/55 [==============================] - 26s 476ms/step - loss: 0.2566 - accuracy: 0.9425 - val_loss: 1.4191 - val_accuracy: 0.7587\n",
      "Epoch 13/20\n",
      "55/55 [==============================] - 28s 519ms/step - loss: 0.2061 - accuracy: 0.9494 - val_loss: 1.6594 - val_accuracy: 0.7373\n",
      "Epoch 14/20\n",
      "55/55 [==============================] - 28s 505ms/step - loss: 0.1589 - accuracy: 0.9580 - val_loss: 1.7024 - val_accuracy: 0.7292\n",
      "Epoch 15/20\n",
      "55/55 [==============================] - 27s 493ms/step - loss: 0.1728 - accuracy: 0.9540 - val_loss: 1.6940 - val_accuracy: 0.7560\n",
      "Epoch 16/20\n",
      "55/55 [==============================] - 27s 499ms/step - loss: 0.1289 - accuracy: 0.9701 - val_loss: 1.5720 - val_accuracy: 0.7587\n",
      "Epoch 17/20\n",
      "55/55 [==============================] - 27s 500ms/step - loss: 0.1394 - accuracy: 0.9672 - val_loss: 2.1230 - val_accuracy: 0.7185\n",
      "Epoch 18/20\n",
      "55/55 [==============================] - 26s 475ms/step - loss: 0.1139 - accuracy: 0.9701 - val_loss: 1.8752 - val_accuracy: 0.7507\n",
      "Epoch 19/20\n",
      "55/55 [==============================] - 27s 487ms/step - loss: 0.1091 - accuracy: 0.9724 - val_loss: 1.8972 - val_accuracy: 0.7185\n",
      "Epoch 20/20\n",
      "55/55 [==============================] - 27s 492ms/step - loss: 0.1055 - accuracy: 0.9718 - val_loss: 1.8553 - val_accuracy: 0.7265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd8758dd80>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.fit(x=X_train_np, y = Y_train_np, validation_data = (X_valid_np, Y_valid_np), epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 167ms/step - loss: 1.8233 - accuracy: 0.7453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8233468532562256, 0.7453083395957947]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_np = np.array(X_test)\n",
    "Y_test_np = np.array(Y_test)\n",
    "\n",
    "classifier_model.evaluate(X_test_np, Y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 2s 192ms/step - loss: 1.8444 - accuracy: 0.7319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8443881273269653, 0.7319034934043884]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_np = np.array(X_test)\n",
    "Y_test_np = np.array(Y_test)\n",
    "\n",
    "classifier_model.evaluate(X_test_np, Y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 3s 210ms/step - loss: 2.1192 - accuracy: 0.7212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.119243621826172, 0.7211796045303345]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_np = np.array(X_test)\n",
    "Y_test_np = np.array(Y_test)\n",
    "\n",
    "classifier_model.evaluate(X_test_np, Y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
